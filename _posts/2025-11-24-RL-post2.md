---
title: 强化学习深度扩展问题
date: 2025-11-22 18:00:00 +0800
categories: [强化学习, 文献阅读笔记]
math: true
---



## 1.背景

在过去，RL的研究人员发现，简单的增加网络深度通常会导致性能下降或训练不稳定。普遍认为，RL的反馈信号稀疏、噪音大，在扩展到大模型时往往面临挑战。

所以，目前的标准做法通常是：

**网络层数**：2-4层全连接层。
**主要算法**：SAC, TD3, PPO等。

---
## 2. 解决方案
### 2024，Farebrother et al.
在深度强化学习中，将价值函数的训练从传统的均方误差回归问题转化为分类问题（使用分类交叉熵损失），可以显著提高算法的性能、鲁棒性以及在大规模神经网络架构下的可扩展性 。


#### A. 核心方法 (HL-Gauss)
文章采用的方法是 **Histogram Loss with Gaussian Smoothing (HL-Gauss)**。

* 该方法利用了回归任务的序数结构，通过高斯平滑将概率质量分配给目标值附近的多个“类别”（bins）。
* 实验结果证明，相比于简单的“Two-Hot”编码（仅将概率分配给最近的两个边界），HL-Gauss 类似于监督学习中的标签平滑，能更有效地利用数据。

#### B. 与分布强化学习 (Distributional RL) 的区别
虽然该方法使用了类似于 C51 的分类表示，但它并非关注建模回报的随机分布，而是关注于使用分类损失函数来训练价值函数本身。

---
### 2025，Kevin et al.
该研究并未提出一种全新的算法范式，而是介绍了将现有的**对比强化学习**框架与现代深度神经网络架构进行系统性融合的方法，以探究深度扩展对强化学习性能的影响。

#### A. 算法基础：对比强化学习(CRL)
本研究使用对比强化学习算法来解决**目标条件强化学习**[👈点击阅读]({% post_url 2025-11-25-RL-post3 %})问题。作为一种**自监督学习**经典算法，CRL运作机制如下：
* 在缺乏外部奖励信号的环境中，智能体必须通过**自主探索机制**，习得到达状态空间中任意指定目标的策略。
* Critic 网络的优化采用 **InfoNCE 损失函数**[👈点击阅读]({% post_url 2025-11-25-RL-post4 %})。这一公式化处理实际上将原本的价值估计问题重构为**分类任务**，即判别给定的状态-动作对是否属于通向目标状态的可能轨迹。

#### B. 网络架构：
为拓展网络深度，本研究引入了关键的**残差连接**结构。

* 每个残差块由 **4 个重复的计算单元**构成。每个单元内部遵循以下特定的层级序列：全连接层、归一化和Swish 激活函数。
    

* **深度：** 指架构中所有残差块内部包含的 Dense 层的总量（即 
  $$ 4 \times N $$
  ）。

* **训练稳定性：** 实验证据表明，**LayerNorm** 与 **Swish 激活函数**的结合对于维持深层网络在训练过程中的数值稳定性及梯度传播至关重要，是实现深度扩展的必要组件。

![Figure 2: Architecture. Our approach integrates
residual connections into both the actor and critic networks of the Contrastive RL algorithm. The depth of this residual architecture is defined as the total number of Dense layers across the residual blocks, which, with our residual block size of 4, equates to 4N.](/assets/img/posts/251124RLpost2/2503.14858.fig2.png){: width="60%" style="display: block; margin: 0 auto;" }


---
## 4. 深层网络的优势：

#### A. 更好地表征

在一个 U 型迷宫中，浅层网络（深度 4）学到的是**欧几里得距离**。“不会绕墙”。

深层网络（深度 64）：学到了**测地线距离**。**Q值热力图**沿着迷宫的走廊弯曲，这意味着深层网络理解了环境的拓扑结构。

![Figure 9: Deeper Q-functions are qualitatively different. The shallow depth 4 network (left) naively relies on Euclidean proximity, showing high Q values near the start despite a maze wall. In contrast, the depth 64 network (right) clusters high Q values at the goal, gradually tapering along the interior.](/assets/img/posts/251124RLpost2/2503.14858.fig9.png){: .shadow width="50%" style="display: block; margin: 0 auto;" }

#### B. 泛化与拼接

深层网络表现出更强的拼接能力，能够将训练中的短路径片段组合起来，解决训练中从未见过的长距离目标任务 。
作者做了一个极端的实验：训练数据中，起点和终点都在 3 步以内。但在测试时，目标在 6 步以外。

![Figure 11: Deeper networks exhibit improved generalization. Top: Training on short paths (green) vs Evaluation on long paths (red). Bottom: Performance comparison showing deep networks succeed where shallow ones fail.](/assets/img/posts/251124RLpost2/2503.14858.fig11.png){: width="60%" style="display: block; margin: 0 auto;" }
_更深的网络展现出更强的泛化能力。（左上）对迷宫环境的训练设置进行修改，使起始-目标对之间的距离≤3个单元。该设计确保训练过程中未遇到评估对（右上），从而通过拼接测试组合泛化能力。（下图）随着网络层数从4层增至16至64层，泛化能力持续提升。_

#### C. 解锁了大 Batch Size 的潜力

在传统 RL 中，增大 Batch Size带来的性能提升并不明显。作者发现，只有当网络**足够深**的时候，增大Batch Size才会带来性能提升。

![Figure 7: Deeper networks unlock batch size scaling. As depth increases from 4 to 64, larger networks can effectively leverage larger batch sizes (darker lines) to achieve further improvements.](/assets/img/posts/251124RLpost2/2503.14858.fig7.jpg){: width="70%" style="display: block; margin: 0 auto;" }

#### D. 探索与表达能力的协同

实验显示，好的结果离不开深层网络和好的数据集。


参考文献：\
[1] Wang K, Javali I, Bortkiewicz M Ĺ, et al. 1000 layer networks for self-supervised rl: Scaling depth can enable new goal-reaching capabilities[J]. arXiv preprint arXiv:2503.14858, 2025.

---
title: 分布式强化学习
date: 2025-11-25 18:00:00 +0800
categories: [强化学习, 概念]
math: true
---


## 1. 背景
强化学习的核心在于运用贝尔曼方程来描述价值函数：
<div align="center">
$$
Q^{\pi}(x, a) = \mathbb{E} R(x, a) + \gamma \mathbb{E}_{P, \pi} Q^{\pi}(x', a').
$$
</div>
目前模型的估计结果是基于策略和状态转移两个概率分布的价值期望。而在这个建模过程中，完整的分布信息很大程度上被丢失了，分布式强化学习就是想要解决这个问题，对累积回报的分布进行建模，而非只建模其期望。

<hr style="border-top: 3px solid red;" />

## 2. 算法分类
### 2.1. 学习离散类别分布

#### A. C51
包含两个关键步骤：

1）**随机分布贝尔曼算子** (Stochastic Distributional Bellman Operator)

在计算目标时，C51 对分布 $Z$ 进行操作，而不是对标量 $Q$ 操作。公式为：

$$
(\hat{\mathcal{T}}Z)(s, a) \stackrel{D}{=} R + \gamma Z(s', a')
$$

这意味着对原本的分布直方图做了两件事：
* **缩放 (Scaling)**：乘以折扣因子 $\gamma$。这会使分布变“窄”，向 0 收缩。
* **平移 (Shifting)**：加上即时奖励 $R$。这会把整个分布在数轴上左右移动。

2）**分类投影** (Categorical Projection)


* **问题**：原来的分布是定义在固定的支点（atoms/bins）$\{z_1, \dots, z_m\}$ 上的。但是，经过缩放和平移后，新的分布支点变成了 $R + \gamma z_i$，这些新位置通常**落不到**原来的固定支点上。

* **投影算法**：
C51 使用线性插值将每一个移动后的点 $R + \gamma z_j$ 的概率质量，分配给它最近的两个固定支点。

$$
\xi_j(x) = \frac{x - z_j}{z_{j+1} - z_j} \dots
$$

![Figure 1: C51的贝尔曼更新](/assets/img/posts/251125RLpost6/C51.png){: width="80%" style="display: block; margin: 0 auto;" }
_Figure 1: C51的贝尔曼更新。(a)策略 π 下的下一状态分布，(b)折扣使分布向0收缩，(c)奖励使其发生偏移，(d)投影步骤。_

### 2.2. 学习概率分布的分位数
#### A. QR-DQN
* **核心思想**：



C51 算法将可能的 Z 划分为51个固定区间，然后学习 Z 到每个区间的概率，它固定了 N 个概率的可能取值，然后使用可学习的、与 N 个值相对应的 N 个概率值去描述分布。C51 实际上最小化的是 KL 散度，无法直接通过随机梯度下降来最小化 Wasserstein 距离。

而 QR-DQN 则使用分位数来描述分布，也就是说，固定 N 个分位数，然后学习这 N 个分位数对应的数值取值。

![Figure 2: QR-DQN使用分位数描述分布](/assets/img/posts/251125RLpost6/QRDQN1.png){: width="60%" style="display: block; margin: 0 auto;" }
_Figure 2: QR-DQN使用分位数描述分布。1-Wasserstein最小化投影至N=4个均匀加权Dirac函数。阴影区域之和构成1-Wasserstein误差。_

----
* **分位数回归损失**：
  
对于某个分位数 $\tau$，如果预测误差是 $u = \text{target} - \text{prediction}$，标准损失为：

$$
\rho_\tau(u) = u(\tau - \mathbb{I}_{\{u<0\}})
$$

为了解决标准分位数损失在 $u = 0$ 处不可导的问题，作者结合了 Huber Loss（在误差很小时是平方误差，误差大时是线性误差）。最终使用的 **Quantile Huber Loss** 公式为：

$$
\rho_\tau^\kappa(u) = |\tau - \mathbb{I}_{\{u<0\}}| \frac{\mathcal{L}_\kappa(u)}{\kappa}
$$

其中 $\mathcal{L}_\kappa(u)$ 是标准的 Huber 损失：

$$
\mathcal{L}_\kappa(u) = \begin{cases} 
\frac{1}{2}u^2, & \text{if } |u| \le \kappa \\
\kappa(|u| - \frac{1}{2}\kappa), & \text{otherwise}
\end{cases}
$$

论文中通常设 $\kappa = 1$。

----
* **算法流程**：
  
QR-DQN 建立在 DQN 架构之上，主要做了以下3处修改：

1）**输出层维度**<br>
将神经网络的输出层大小调整为 $|\mathcal{A}| \times N$ 。其中 $|\mathcal{A}|$ 是动作数量，$N$ 是分位数目标（quantile targets）的数量（超参数）。

2）**损失函数**<br>
将 DQN 原本使用的 Huber Loss 替换为 **Quantile Huber Loss**（分位数 Huber 损失，通常设 $\kappa=1$）。

3）**优化器**<br>
将原本的 RMSProp 优化器替换为 **Adam** 优化器。

![Figure 3: QR-DQN算法流程](/assets/img/posts/251125RLpost6/QRDQN2.png){: width="60%" style="display: block; margin: 0 auto;" }
_Figure 3: QR-DQN算法流程。_

---
#### B. IQN

* **核心思想**：

不固定概率，也不固定位置。它学习的是从概率 $\tau \in [0, 1]$ 到价值回报的映射函数。通过输入任意的 $\tau$，网络都能给出对应的价值估计。

![Figure 4: 四种算法比较](/assets/img/posts/251125RLpost6/IQN1.png){: width="60%" style="display: block; margin: 0 auto;" }
_Figure 4: 四种算法比较。_

---
* **算法流程**：
  
网络输入分为两个部分：

* $\psi(x)$: 由卷积层计算出的状态特征向量 （对应传统的 DQN 部分）。
* $\phi(\tau)$: 由一个额外的函数生成的**采样点嵌入 (Embedding)**，它将标量 $\tau \in [0, 1]$ 映射为向量。


1）**分位数特征提取**：<br>
将标量 $\tau$ 映射为嵌入向量 $\phi(\tau)$。

$$\phi_{j}(\tau) := \text{ReLU}(\sum_{i=0}^{n-1} \cos(i\pi\tau) w_{ij} + b_j)$$

这里使用余弦基函数将 $\tau$ 扩展到高维空间。

2）**特征融合**：

$$ Z_{\tau}(x, a) \approx f(\psi(x) \odot \phi(\tau))_a $$

其中$$\odot$$表示逐元素哈达玛乘积。

3）**采样分位数回归误差**：<br>
由于 $\tau$ 是连续的，无法计算所有 $\tau$ 的损失。IQN 采用**采样近似**的方式：

对于每次更新，从 $U([0,1])$ 中采样 $N$ 个 $\tau$ 用于计算当前网络输出，采样 $N'$ 个 $\tau'$ 用于计算目标网络输出。
计算所有采样对 $(i, j)$ 之间的 **Quantile Huber Loss** 并求平均：

$$\mathcal{L} \approx \frac{1}{N'} \sum_{i=1}^{N} \sum_{j=1}^{N'} \rho_{\tau_i}^\kappa (\mathcal{T}Z_{\tau'_j}(x', a^*) - Z_{\tau_i}(x, a))$$

其中 $\mathcal{T}$ 是贝尔曼算子，$\rho$ 是非对称的分位数 Huber 损失函数。

<hr style="border-top: 3px solid red;" />

## 3. 应用
### 3.1. 多目标

现有的多变量 DRL 方法存在缺少理论支撑、Oracle难以计算和对完整的联合分布进行建模困难等问题。当奖励维度 $d > 1$ 时，标准的分类 TD 学习分析方法效果较差。
Wiltzer等提出了第一个**无需 Oracle 且计算可行**的算法，并证明了其在动态规划（DP）和时序差分（TD）学习中的收敛性。

该研究证明了多变量分布 Bellman 算子 $\mathcal{T}^{\pi}$ 在特定的**最大均值差异（MMD）**度量下是一个压缩映射，并提出了**基于粒子的方法 (Equally-Weighted Particles)**和**基于分类的方法 (Categorical / Signed Measures)**两种主要的算法实现。

* EWP是一种**随机化动态规划**算法。通过从更新后的分布中采样粒子来通过投影步骤。实验结果表明该方法在**高概率**下收敛，且误差界限与粒子数 $m$ 和维度 $d$ 的多项式相关。

* Signed-Cat-TD算法将表示空间从概率单纯形**放宽到“质量为1的有符号测度（Mass-1 Signed Measures）”空间**。这使得投影操作变成仿射的（Affine），从而可以利用标准的随机逼近理论证明收敛。

### 3.2. 风险敏感控制
Lim等利用分布式强化学习解决基于CVaR风险度量的风险敏感策略学习问题。该研究证明应用分布式贝尔曼最优算子时，标准的动作选择策略既不能收敛到动态马尔可夫CVaR，也不能收敛到静态非马尔可夫CVaR。基于此，该研究提出了一种改进算法，引入了新的分布式贝尔曼算子，并证明该策略极大地扩展了分布式强化学习在学习和表示CVaR优化策略方面的效用。[👈点击阅读]({% post_url 2025-11-25-RL-post7 %})

### 3.3. 神经科学
Dabney等受分布强化学习的研究启发，提出了一种多巴胺奖励预测误差（RPE）的价值分布模型，相关成果发表在2020年《Nature》。

多巴胺神经元放电的瞬时变化与大脑的奖励预测紧密相关，多项研究表明，多巴胺能神经元对意外奖励有反应，对完全预测的奖励没有反应，而对未出现的预测奖励的反应低于基线水平。该研究提出大脑并非将预测奖励表征为一个均值，而是表征为一个概率分布，该奖励预测误差模型在细胞实验和小鼠实验中得到了验证。
![Figure 5: 四种算法比较](/assets/img/posts/251125RLpost6/dopamine1.png){: width="60%" style="display: block; margin: 0 auto;" }
_Figure 5:a, 经典和分布TD模拟产生的RPE。每个水平条代表一个神经元。每个点的颜色对应一个特定的奖励幅度。x 轴表示细胞在获得奖励时放电率的变化。在经典TD中，所有细胞的RPE信号大致相同。在分布TD中，细胞的兴奋程度存在显著差异,一些细胞对几乎所有奖励都做出积极反应，而另一些细胞则仅对最大的奖励做出积极反应。b, 记录行为小鼠中光识别多巴胺神经元的反应。_

参考论文：\
[1] Bellemare M G, Dabney W, Munos R. A distributional perspective on reinforcement learning[C]//International conference on machine learning. PMLR, 2017: 449-458.\
[2] Dabney W, Rowland M, Bellemare M, et al. Distributional reinforcement learning with quantile regression[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).\
[3] Dabney W, Ostrovski G, Silver D, et al. Implicit quantile networks for distributional reinforcement learning[C]//International conference on machine learning. PMLR, 2018: 1096-1105.\
[4] Wiltzer H, Farebrother J, Gretton A, et al. Foundations of multivariate distributional reinforcement learning[J]. Advances in Neural Information Processing Systems, 2024, 37: 101297-101336.\
[5] Dabney W, Kurth-Nelson Z, Uchida N, et al. A distributional code for value in dopamine-based reinforcement learning[J]. Nature, 2020, 577(7792): 671-675.\
[6] Lim S H, Malik I. Distributional reinforcement learning for risk-sensitive policies[J]. Advances in Neural Information Processing Systems, 2022, 35: 30977-30989.


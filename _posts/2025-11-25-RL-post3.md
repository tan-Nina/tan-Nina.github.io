---
title: 目标条件强化学习
date: 2025-11-25 18:00:00 +0800
categories: [强化学习, 概念]
math: true
---


在传统的强化学习（Standard RL）中，智能体（Agent）通常只有一个固定的任务。
* **例子**：训练一个机器人只学会**走出某一个特定的迷宫**。
* **局限**：如果你换了一个出口，或者换了一个迷宫，它就必须从头重新学习。



**目标条件强化学习（GCRL）**则不同。它要求智能体不仅要观察当前的**状态（State）**，还要接收一个**目标（Goal）**作为输入。


**数学定义：**
* **策略（Policy）**：
  $$\pi(a|s,g)$$
  动作 $$a$$ 取决于**状态 $$s$$ 和目标 $$g$$** 。
* **奖励（Reward）**：通常非常简单且稀疏（Sparse）。例如：如果你到达了 $g$ 附近，奖励为 1，否则为 0。

**GCRL 的挑战**：
由于奖励非常稀疏（只有到了终点才有反馈），在大规模环境中，智能体很容易迷失，很难通过随机探索获得奖励信号。这就需要更高效的学习信号——比如InfoNCE损失函数[👈点击阅读]({% post_url 2025-11-25-RL-post4 %})。

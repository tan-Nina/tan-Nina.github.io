---
title: 逆强化学习
date: 2025-11-25 12:00:00 +0800
categories: [强化学习, 概念]
math: true
---
## 1. 背景
逆强化学习的目标是：观察一个智能体在环境中的行为后，恢复奖励函数。

面临着两个根本性的理论挑战。

1. **奖励函数的歧义性**（Reward Ambiguity）。在MDP中，对于任意给定的专家行为轨迹，存在无数种奖励函数可以解释该行为不仅是合理的，甚至是“最优”的。最极端的例子是，如果奖励函数在所有状态下均为零，那么任何策略都是最优的。早期的解决方案，如最大边际规划，试图通过寻找一个能使专家策略与次优策略之间产生最大“边际”的奖励函数来解决这一问题。然而，这种方法通常假设专家是完美的或接近完美的，难以处理现实世界数据中普遍存在的噪声和次优性。

2. 其次，是如何**处理环境的随机性与信息的时间序列性**。Ziebart在其2008年的开创性工作MaxEnt IRL中引入了概率视角，假设专家产生的轨迹服从玻尔兹曼分布，其概率与轨迹的累积奖励成指数正比。这一假设虽然解决了奖励歧义性问题（通过选择熵最大的分布来消除不确定性），但它隐含了一个致命的缺陷：它假设整个轨迹是在初始时刻一次性决定的，即智能体预知了未来的所有状态转移。这将导致在高度随机的环境中，模型预测会出现偏差 。

<hr style="border-top: 3px solid red;" />
## 2. 最大因果熵原理
### 2.1. 理论

与标准最大熵原理**最大化联合分布** $P(\tau)$ **的熵不同**，最大因果熵关注的是**策略的条件熵**。

具体而言，它要求在每一个时间步 $t$ ，
智能体在给定历史信息 $h_t$ 的情况下选择动作 $a_t$ 的概率分布 $$P(a_t|h_t)$$ ，应当使得在满足特征期望匹配约束的前提下，策略对未来动作序列的因果熵最大化。数学上，这意味着策略应当满足：

<div align="center">
$$\pi(a_t|s_t) \propto \exp(Q_{soft}(s_t, a_t) - V_{soft}(s_t))$$
</div>

其中，$Q_{soft}$ 和 $V_{soft}$ 是基于“软最大化”（Soft Maximization, Log-Sum-Exp）算子的值函数。这一公式推导出的策略不仅能够解释专家数据的特征统计量，还能提高系统的鲁棒性，从而避免了过拟合。

---
### 2.2. Guided Cost Learning
Finn等针对IOC中设计代价函数需要有效提取特征和正则化的挑战，提出了一种无需精细调参即可学习任意非线性代价函数（如神经网络）的算法；此外，针对高维连续系统在未知动态环境中学习代价函数的难题，构建了**基于样本的高效近似方法**来实现最大熵逆最优控制。在一系列模拟任务和真实机器人操作问题中，所提出的方法在任务复杂度和样本效率方面均展现出一定优势。

#### A. 自适应采样与策略优化

在最大熵IOC中，估计配分函数 $Z$ 需要从背景分布中采样。采样分布 $q(\tau)$ 选择是关键因素。

该研究提出不使用固定的背景分布，使用基于局部线性模型（Time-varying linear models）的策略搜索算法不断调整采样分布 $q(\tau)$。

#### B. 神经网络成本函数
使用深层神经网络来参数化成本函数 $c_{\theta}(x_t, u_t)$。为避免NN过拟合，设计了两个正则化项：

<div align="center">
$$g_{lcr}(\tau)=\sum_{x_{t}\in\tau}[(c_{\theta}(x_{t+1})-c_{\theta}(x_{t}))-(c_{\theta}(x_{t})-c_{\theta}(x_{t-1}))]^{2}$$
</div>

<div align="center">
$$g_{mono}(\tau)=\sum_{x_{t}\in\tau}[max(0,c_{\theta}(x_{t})-c_{\theta}(x_{t-1})-1)]^{2}$$
</div>

![Figure 1: GCL策略优化和数据收集算法流程](/assets/img/posts/251125RLpost8/GCL.png){: width="60%" style="display: block; margin: 0 auto;" }
_Figure 1: GCL策略优化和数据收集算法流程。_

## 3.非对抗式
### 3.1. SFM
Jain等提出了一种名为“成功者特征匹配”（Successor Feature Matching, SFM）的新方法。不同于 GAIL 等方法需要训练一个判别器来区分专家和智能体的数据，也不需要双层优化结构。SFM 直接通过策略梯度下降来最小化智能体与专家特征之间的差距。

作者证明，在特征匹配的目标下，最优的奖励权重 $w^*$ 可以直接近似为专家的 successor feature $\hat{\psi}^E$ 与智能体的 successor feature $\hat{\psi}^\pi$ 之差。


$$w_{\pi\rightarrow\pi_{E}}^{*} \propto \hat{\psi}^{E} - \hat{\psi}^{\pi}$$

多个控制任务的表现结果表明，SFM在使用单个示范时能够有效学习并超过传统的行为克隆及其他对抗式方法，表现出更好的样本效率和更低的最优性间隙。



参考论文：\
[1] Ziebart B D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy[M]. Carnegie Mellon University, 2010.\
[2] Finn C, Levine S, Abbeel P. Guided cost learning: Deep inverse optimal control via policy optimization[C]//International conference on machine learning. PMLR, 2016: 49-58.\
[3] Garg D, Chakraborty S, Cundy C, et al. Iq-learn: Inverse soft-q learning for imitation[J]. Advances in Neural Information Processing Systems, 2021, 34: 4028-4039.\
[4] Jain A K, Wiltzer H, Farebrother J, et al. Non-adversarial inverse reinforcement learning via successor feature matching[J]. arXiv preprint arXiv:2411.07007, 2024.\
[5] Baimukashev D, Alcan G, Kyrki V. Automated Feature Selection for Inverse Reinforcement Learning[J]. arXiv preprint arXiv:2403.15079, 2024.

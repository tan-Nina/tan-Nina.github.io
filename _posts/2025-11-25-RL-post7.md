---
title: 值分布在风险评估中的应用
date: 2025-11-25 18:00:00 +0800
categories: [强化学习, 文献阅读笔记]
math: true
---
## 1. 概念
### 1.1. Distributional RL

1. 分布贝尔曼方程（Distributional Bellman equation）
$$\tilde{\mathcal{T}} U(x, a) \overset{D}{=} R + \gamma U(X', A'), \quad A' = \arg \max_{a'} \mathbb{E} [U(X', a')]$$

2. 分布贝尔曼算子 (Distributional Bellman operator)
$$\tilde{\mathcal{T}}^\pi U(x, a) \overset{D}{=} R + \gamma U(X', \pi(X'))$$

其中 $\tilde{\mathcal{T}}^\pi$ 是 distributional Bellman 算子，且 $\tilde{\mathcal{T}}^\pi$ 是 $\gamma$-contraction（$\gamma$-压缩映射）。但是，distributional bellman 最优算子 $\tilde{\mathcal{T}}^*$ 不是 $\gamma$-contraction 的。

---

### 1.2. Conditional Value-at-risk (CVaR)

1. $Z$ 离散：
$$C_\alpha(Z) := \max_{s \in \mathbb{R}} s - \frac{1}{\alpha}\mathbb{E}[(s - Z)^+]$$
其中，$(x)^+ = \max\{x, 0\}$。

2. $Z$ 连续：
$$C_\alpha(Z) = \mathbb{E}[Z \mid Z < q_\alpha(Z)]$$
其中，$q_\alpha(Z) := \inf\{s : \Pr(Z \le s) \ge \alpha\}$。


---

### 1.3. Static CVaR

**目标：找到最大化 $C_\alpha(Z^\pi)$的策略:**

$$\begin{aligned}\max_\pi \max_s s - \frac{1}{\alpha}\mathbb{E}^\pi_p \left[(s - Z^\pi)^+\right]\end{aligned}$$

---

### 1.4. Dynamic CVaR

从动态规划的角度看，一个替代的、时间一致的或马尔可夫版本的 CVaR 可能更为方便。这类风险衡量标准是由 Ruszczynski 提出的，作者将该类 CVaR 称为动态 CVaR (Dynamic CVaR)，其定义为递归式：

$$
\begin{aligned}
\forall \pi, x, a, \quad & D^\pi_{\alpha, 0}(x, a) := C_\alpha[r_t | x_t = x, a_t = a], \\
\forall \pi, x, a, T > 0, \quad & D^\pi_{\alpha, T}(x, a) := C_\alpha[r_t + \gamma D^\pi_{\alpha, T-1}(x_{t+1}, \pi(x_{t+1})) | x_t = x, a_t = a], \\
\forall \pi, x, a, \quad & D^\pi_{\alpha}(x, a) := \lim_{T \to \infty} D^\pi_{\alpha, T}(x, a).
\end{aligned}
$$

<hr style="border-top: 3px solid red;" />
## 2. 核心观点
### 2.1. 观点1：
**应用分布式贝尔曼最优算子时，标准的动作选择策略可能既不能收敛到动态马尔可夫CVaR，也不能收敛到静态非马尔可夫CVaR。**

**反例证明**：

 $\{(p_1; r_1), (p_2; r_2), ...\}$ 表示一个随机变量，该变量以概率 $p_1$ 取值 $r_1$，以概率 $p_2$ 取值 $r_2$，依此类推。

假设 $\gamma=1$，但通过对奖励进行简单的缩放，所述实例**适用于任何 $\gamma$**。设 $p, \epsilon$ 满足 $0 < \epsilon \ll p < 1 - \epsilon$。假设所选的 CVaR 水平 $\alpha$ 满足$p^2 + \epsilon < \alpha < p$ 。只考虑**确定性策略**情形，因为动态 CVaR 和静态 CVaR 都存在确定性的最优策略。

对于图 1(a) 中的 MDP，设 $$X_1$$ 为初始状态。只有两种可能的策略：

1. 在状态 $X_1$ 选择动作 $A_1$ 或 $A_2$。很容易看出，对于 $\alpha < p$，有 $$D^*_\alpha(X_2) = 0$$ 且 $$D^*_\alpha(X_1) = 0$$，因此最优的动态 CVaR 策略是 $A_2$。

2. 然而，对于任意 $$U \in \mathcal{Z}$$，$$\tilde{\mathcal{T}}^D_\alpha U$$ 收敛于 $$U^*(X_1, A_1) = \{(p^2; 0), (1 - p^2; 1)\}$$，因此对于 $$\alpha > p^2 + \epsilon$$，有 $$C_ \alpha [U^*(X_1, A_1)] > \epsilon$$，从而策略会选择 $A_1$。


![Figure 1: 反例](/assets/img/posts/251125RLpost7/CA1.jpg){: width="90%" style="display: block; margin: 0 auto;" }
_Figure 1: (a) $$D^*_\alpha$$ 低估真实 CVaR 的示例。(b) $$D^*_\alpha$$ 高估真实 CVaR 的示例。(c)马尔可夫动作选择策略导致低估真实 CVaR 的示例。_

### 2.2. 方法：
为了解决上述问题，论文提出了一种新的方法，利用学习到的分布来模拟决策过程。

* 要优化静态 CVaR，通常需要引入一个额外的状态变量 $s$（阈值），用来记录“还需要多少奖励才能满足风险要求”。
* 论文提出了一个**新的分布 Bellman 算子**($\tilde{\mathcal{T}}_{\psi}$)。与传统算子不同，它在选择目标动作（Target Action）时，不是简单地最大化当前的 CVaR，而是根据当前累积奖励的阈值 $s$ 来选择能最大化 $W^U$（风险调整后价值）的动作。
    * 公式核心逻辑：$A' = \arg \max_{a'} W^U(X', \frac{\psi(x)-R}{\gamma}, a')$ 。

![Figure 2: 静态 CVaR 的策略执行算法流程](/assets/img/posts/251125RLpost7/CA2.png){: width="80%" style="display: block; margin: 0 auto;" }
_Figure 2: 静态 CVaR 的策略执行算法流程。_

![Figure 3: 用于静态 CVaR 的分位数回归分布 Q 学习算法流程](/assets/img/posts/251125RLpost7/CA3.png){: width="80%" style="display: block; margin: 0 auto;" }
_Figure 3: 用于静态 CVaR 的分位数回归分布 Q 学习算法流程。_


参考论文：\
[1] Lim S H, Malik I. Distributional reinforcement learning for risk-sensitive policies[J]. Advances in Neural Information Processing Systems, 2022, 35: 30977-30989.

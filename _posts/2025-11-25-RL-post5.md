---
title: 值分布在可扩展的深度强化学习中的应用
date: 2025-11-25 18:00:00 +0800
categories: [强化学习, 文献阅读笔记]
math: true
---

### 1. 核心问题：值函数扩展到值分布。
在强化学习中，价值函数或回报可能出现较大的动态范围，如果直接用回归的方式预测这些值，容易出现：
* 数值范围过大导致梯度不稳定
* 使用同一回归模型完成不同任务时，需要额外对价值函数或回报进行缩放，但是缩放因子的选择需要依据环境而调整。

**挑战**：回归的损失函数是计算两个标量间的MSE，而交叉熵是计算两个分布间的距离，因此要做的事情是将原本标量的 $Q$ 和 $Q_{target}$ 用分布表示出来。

![Figure 1: 在基于交叉熵的TD学习中可视化目标值的分类分布。](/assets/img/posts/251124RLpost2/2403.03950.fig2.jpg){: width="80%" style="display: block; margin: 0 auto;" }
_Figure 1: 在基于交叉熵的学习中可视化目标值的分类分布。_
* **Two-Hot**（左）将概率质量精确地分布在两个位置
* **HL-Gauss**（中）将概率质量分布到相邻位置
* **CDRL**（右）建模分类回报分布，按比例将概率质量分布到相邻位置
* 
### 2. 方法一：Two-Hot Categorical Distribution

* **原理**：找到包围目标值的两个相邻支点 $z_i$ 和 $z_{i+1}$（即 $z_i \le \text{Target} \le z_{i+1}$）。
* **计算**：根据目标值与这两个支点的距离线性分配概率。目标离哪个点越近，该点的概率就越大。
 <div align="center">
$$
p_{i}(S_{t},A_{t};\theta^{-}) = \frac{(\hat{\mathcal{T}}Q)(S_{t},A_{t};\theta^{-}) - z_{i}}{z_{i+1} - z_{i}} 
$$
$$
\quad p_{i+1}(S_{t},A_{t};\theta^{-}) = \frac{z_{i+1} - (\hat{\mathcal{T}}Q)(S_{t},A_{t};\theta^{-})}{z_{i+1} - z_{i}}
$$
</div>

* **缺点**：**没有充分利用离散回归的“序数结构”（Ordinal Structure）**，即它认为各个类别是独立的，忽略了类别之间的邻域关系。

### 3. 方法二：Histogram Loss (HL-Gauss)
旨在利用回归任务中的序数结构。

* **原理**：类似于监督学习中的**标签平滑（Label Smoothing）**，该方法将概率质量分配给目标附近的多个离散区域，而不仅仅是最近的两个。
* **算法**：
    1.  假设一个围绕目标值的随机变量 $Y$（通常假设为高斯分布），其均值等于目标值。
    2.  将这个分布投影到直方图上。具体做法是：对于每一个bin $z_i$，计算该分布在离散区间 $[z_i - \varsigma/2, z_i + \varsigma/2]$ 内的积分（即累积分布函数 CDF 的差值），作为该类别的概率。
    <div align="center">
   $$
    \begin{aligned} p_{i}(S_{t},A_{t};\theta^{-}) &= \int_{z_{i}-\varsigma/2}^{z_{i}+\varsigma/2} f_{Y|S_{t},A_{t}}(y|S_{t},A_{t}) dy \\ &= F_{Y|S_{t},A_{t}}(z_{i}+\varsigma/2|S_{t},A_{t}) - F_{Y|S_{t},A_{t}}(z_{i}-\varsigma/2|S_{t},A_{t}) \end{aligned} 
   $$
   </div>
* **HL-Gauss**：文中具体采用高斯分布 $N(\mu=\text{Target}, \sigma^2)$ 来生成目标分布，其中 $\sigma$ 是一个控制平滑程度的超参数。

### 4. 方法三：分类分布强化学习(Categorical Distributional RL)
类别模型 $Z$ 来对未来回报的分布进行建模，类似 **Distributional RL (Bellemare et al., 2023)** 。

值得注意的是，作为早期值分布强化学习方法的代表，**C51 (Bellemare et al., 2017)** 使用了类别表示，并最小化“预测分布 $Z$”与“TD 目标分布的模拟”之间的交叉熵 (Cross-entropy)。为此，该研究也研究了将 C51 作为 Two-Hot 和 HL-Gauss 之外的另一种构建交叉熵目标分布的方法。



对类别回报分布进行建模的第一步是定义 $Z$ 上类似的**随机分布式贝尔曼算子 (Stochastic Distributional Bellman Operator)**：

$$
(\widehat{\mathcal{T}}Z)(s, a; \theta^-) \stackrel{D}{=} \sum_{i=1}^m \hat{p}_i(S_{t+1}, A_{t+1}; \theta^-) \cdot \delta_{R_{t+1} + \gamma z_i} \quad \Bigg| \quad S_t = s, A_t = a,
$$

其中 $A_{t+1} = \arg \max_{a'} Q(S_{t+1}, a')$。

随机分布式贝尔曼算子具有**平移 (Shifting)** 和**缩放 (Scaling)** 位置 $z_i$ 的效果，这使得我们需要引入由 Bellemare et al. (2017) 首次提出的**类别投影 (Categorical Projection)**。

从宏观角度来看，这种投影将概率按比例分配给直接相邻的位置 $z_{j-1} \le R_{t+1} + \gamma z_i \le z_j$（参见 Figure 1 右图）。为了帮助我们确定这些相邻位置，我们将向下取整和向上取整定义为：
* $\lfloor x \rfloor = \arg \max \{z_i : z_i \le x\}$
* $\lceil x \rceil = \arg \min \{z_i : z_i \ge x\}$

现在，位置 $z_i$ 的概率可以写为：

$$
p_i(S_t, A_t; \theta^-) = \sum_{j=1}^m \hat{p}_j(S_{t+1}, A_{t+1}; \theta^-) \cdot \xi_j(R_{t+1} + \gamma z_i) \tag{3.4}
$$

其中投影核函数 $\xi_j(x)$ 定义为：

$$
\xi_j(x) = \frac{x - z_j}{z_{j+1} - z_j}\mathbb{1}\{\lfloor x \rfloor = z_j\} + \frac{z_{j+1} - x}{z_{j+1} - z_j}\mathbb{1}\{\lceil x \rceil = z_j\}.
$$



参考论文：Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
